# -*- coding: utf-8 -*-
"""2312514.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RPIx-_xLlEqbIh88ofpCu2va8mW0w3Pj

Lets import All the libraries
"""

import numpy as np
import os
import pickle
import pandas as pd

"""Now we will put student ID in a variable"""

student_id = 2312514

"""Now we will set Seed for all libraries"""

# set same seeds for all libraries

#numpy seed
np.random.seed(student_id)

"""Allowing the GDrive access"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

import os
import shutil

# Add your code to initialize GDrive and data and models paths

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = './CE807-24-SP/Assignment/'
GOOGLE_DRIVE_PATH = os.path.join('/content/gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)

# Create directory if it doesn't exist
if not os.path.exists(GOOGLE_DRIVE_PATH):
    os.makedirs(GOOGLE_DRIVE_PATH)

print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'data', '4') # Make sure to replace 0 with last digit of your student Regitration number

# Create directory if it doesn't exist
if not os.path.exists(DATA_PATH):
    os.makedirs(DATA_PATH)

train_file = os.path.join(DATA_PATH, 'train.csv')
print('Train file: ', train_file)

val_file = os.path.join(DATA_PATH, 'valid.csv')
print('Validation file: ', val_file)

test_file = os.path.join(DATA_PATH, 'test.csv')
print('Test file: ', test_file)


MODEL_PATH = os.path.join(GOOGLE_DRIVE_PATH, 'model', str(student_id)) # Make sure to use your student Regitration number

# Create directory if it doesn't exist
if not os.path.exists(MODEL_PATH):
    os.makedirs(MODEL_PATH)

MODEL_Gen_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Gen') # Model Generative directory
print('Model Generative directory: ', MODEL_Gen_DIRECTORY)

# Create directory if it doesn't exist
if not os.path.exists(MODEL_Gen_DIRECTORY):
    os.makedirs(MODEL_Gen_DIRECTORY)

MODEL_Gen_File = MODEL_Gen_DIRECTORY + '.zip'


MODEL_Dis_DIRECTORY = os.path.join(MODEL_PATH, 'Model_Dis') # Model Discriminative directory
print('Model Discriminative directory: ', MODEL_Dis_DIRECTORY)

# Create directory if it doesn't exist
if not os.path.exists(MODEL_Dis_DIRECTORY):
    os.makedirs(MODEL_Dis_DIRECTORY)

MODEL_Dis_File = MODEL_Dis_DIRECTORY + '.zip'

"""NOw we will see train file."""

train_df = pd.read_csv(train_file)
train_df.head()

"""We are going to use different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation. We will print all the matrics and display Confusion Matrix with proper X & Y axis labels"""

import pandas as pd
from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,
                             confusion_matrix)
import matplotlib.pyplot as plt
import seaborn as sns

def compute_performance(y_true, y_pred):
    """
    Prints different performance metrics like Accuracy, Recall (macro), Precision (macro), and F1 (macro).
    This also displays the Confusion Matrix with proper X & Y axis labels.
    Also, returns the F1 score.

    Args:
        y_true: numpy array or list
            True labels.
        y_pred: numpy array or list
            Predicted labels.

    Returns:
        float
            F1 score
    """

    # Compute performance metrics
    accuracy = accuracy_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred, average='macro')
    precision = precision_score(y_true, y_pred, average='macro')
    f1 = f1_score(y_true, y_pred, average='macro')

    # Print performance metrics
    print("Accuracy:", accuracy)
    print("Recall (macro):", recall)
    print("Precision (macro):", precision)
    print("F1 (macro):", f1)

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Display confusion matrix with proper axis labels
    labels = sorted(np.unique(y_true))
    cm_df = pd.DataFrame(cm, index=labels, columns=labels)

    plt.figure(figsize=(10, 7))
    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.show()

"""To save model

"""

def save_model(model,model_dir):
  # save the model to disk
  # Check if the Model directory exists

  # Note you might have to modify this based on your requirement

  if not os.path.exists(model_dir):
      # Create the directory if it doesn't exist
      os.makedirs(model_dir)
      print(f"Directory '{model_dir}' created successfully.")
  else:
      print(f"Directory '{model_dir}' already exists.")

  model_file = os.path.join(model_dir, 'model.sav')
  pickle.dump(model, open(model_file, 'wb'))

  print('Saved model to ', model_file)

  return model_file

def load_model(model_file):
    # load model from disk

    # Note you might have to modify this based on your requirement

    model = pickle.load(open(model_file, 'rb'))

    print('Loaded model from ', model_file)

    return model

"""Let's Download GDrive link into the directory

"""

import requests

def extract_file_id_from_url(url):
    # Extract the file ID from the URL
    file_id = None
    if 'drive.google.com' in url:
        file_id = url.split('/')[-2]
    elif 'https://docs.google.com' in url:
        file_id = url.split('/')[-1]

    return file_id

def download_file_from_drive(file_id, file_path):
    # Construct the download URL
    download_url = f"https://drive.google.com/uc?id={file_id}"

    # Download the file
    response = requests.get(download_url)
    if response.status_code == 200:
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print("File downloaded successfully!",file_path)
    else:
        print("Failed to download the file.")

def download_zip_file_from_link(file_url,file_path):

  file_id = extract_file_id_from_url(file_url)
  if file_id:
      download_file_from_drive(file_id, file_path)
  else:
      print("Invalid Google Drive URL.")

"""# Method Generative Start
Method 1: Variational Autoencoder (VAE)

First of all we strat with reading data
"""

import pandas as pd

def read_data(file_path):
    # Read data from file
    data = pd.read_csv(train_file)
    return data

# Example usage
train_data = read_data("train.csv")
validation_data = read_data("validation.csv")
test_data = read_data("test.csv")

"""Data Cleaning (if any)
Since Naive Bayes is relatively robust to outliers and missing values, we may not need explicit data cleaning for this method.

No we Convert Data to Vector/Tokenization/Vectorization
python
"""

from sklearn.feature_extraction.text import CountVectorizer

def tokenize_data(data):
    # Extract textual data from the 'comment' column
    X = data['comment']
    y = data['toxicity']
    return X, y

# Example usage
X_train, y_train = tokenize_data(train_data)
X_validation, y_validation = tokenize_data(validation_data)
X_test, y_test = tokenize_data(test_data)

"""Now we Model Declaration/Initialization/Building"""

from sklearn.naive_bayes import MultinomialNB
import joblib
def initialize_model():
    # Initialize Naive Bayes classifier
    model = MultinomialNB()
    return model


# Example usage
model_Gen = initialize_model()

"""## Training Generative Method Code


"""

def train_Gen(train_file, val_file, model_dir):
    """
    Takes train_file, val_file and model_dir as input.
    It trained on the train_file datapoints, and validate on the val_file datapoints.
    While training and validating, it print different evaluataion metrics and losses, wheverever necessary.
    After finishing the training, it saved the best model in the model_dir.

    ADD Other arguments, if needed.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory



    """

    ##########################################################################
    #                     TODO: Implement this function                      #
    ##########################################################################
    # Train the model
# Read training and validation data
    train_data = pd.read_csv(train_file)
    val_data = pd.read_csv(val_file)

    # Tokenize data
    X_train, y_train = tokenize_data(train_data)
    X_val, y_val = tokenize_data(val_data)

    # Initialize the model
    model = initialize_model()

    # Train the model
    model.fit(X_train, y_train)

    # Validate the model
    validation_accuracy = model.score(X_val, y_val)
    print("Validation Accuracy:", validation_accuracy)

    # Save the trained model
    save_model(model, model_dir)

    return validation_accuracy


    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################

"""## Testing Method 1 Code

"""

def test_Gen(test_file, MODEL_PATH,model_gdrive_link):
    """
    take test_file, model_file and output_dir as input.
    It loads model and test of the examples in the test_file.
    It prints different evaluation metrics, and saves the output in output directory

    ADD Other arguments, if needed

    Args:
        test_file: test file name
        model_gdrive_link: GDrive URL
        MODEL_PATH: Directory of Model

    """

    ##########################################################################
    #                     TODO: Implement this function                      #
    ##########################################################################
   # Read test data
    test_data = pd.read_csv(test_file)

    # Tokenize test data
    X_test, y_test = tokenize_data(test_data)

    # Load the trained model
    loaded_model = load_model(MODEL_PATH)

    # Test the model
    test_accuracy = test_model(loaded_model, X_test, y_test)
    print("Test Accuracy:", test_accuracy)

    # Save the output in the output directory (optional)
    # Example: output_data.to_csv(output_dir, index=False)

    return test_accuracy

    ##########################################################################
    #                            END OF YOUR CODE                            #
    ##########################################################################

"""Method Discriminative Start

Method 2: SVM

First we read data
"""

import pandas as pd

def read_data(file_path):
    # Read data from file
    data = pd.read_csv(train_file)
    return data

# Example usage
train_data = read_data("train.csv")
validation_data = read_data("validation.csv")
test_data = read_data("test.csv")

"""Now we clean data"""

# Data cleaning (if any)

def clean_data(data):
    """
    Perform data cleaning if required.

    Parameters:
    - data (DataFrame): The input data.

    Returns:
    - DataFrame: The cleaned data.
    """
    # Perform cleaning operations here if necessary
    return data

"""Now we Convert data to vector/tokenization/vectorization"""

# Convert data to vector/tokenization/vectorization
from sklearn.feature_extraction.text import TfidfVectorizer

def tokenize_and_vectorize(data):
    """
    Tokenize and vectorize the text data.

    Parameters:
    - data (DataFrame): The input data.

    Returns:
    - tuple: Tuple containing the vectorized features and the target variable.
    """
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(data['comment'])
    y = data['toxicity']
    return X, y

"""Model Declaration/Initialization/building"""

# Model Declaration/Initialization/building
from sklearn.svm import SVC

def initialize_model():
    """
    Initialize the SVM model.

    Returns:
    - model: The initialized SVM model.
    """
    model = SVC()
    return model

# Example usage:
model = initialize_model()

"""Now we train our SVM model"""

def train_dis(train_file, val_file, model_dir):
    """
    Takes train_file, val_file and model_dir as input.
    It trains on the train_file datapoints, and validates on the val_file datapoints.
    While training and validating, it prints different evaluation metrics and losses, wherever necessary.
    After finishing the training, it saves the best model in the model_dir.

    Args:
        train_file: Train file name
        val_file: Validation file name
        model_dir: Model output Directory

    Returns:
        model_gdrive_link: Google Drive link to the saved model
    """

    # Read train and validation data
    train_data = read_data(train_file)
    val_data = read_data(val_file)

    # Clean data
    cleaned_train_data = clean_data(train_data)
    cleaned_val_data = clean_data(val_data)

    # Tokenize and vectorize
    X_train, y_train = tokenize_and_vectorize(cleaned_train_data)
    X_val, y_val = tokenize_and_vectorize(cleaned_val_data)

    # Initialize model
    model = initialize_model()

    # Train and validate model
    val_accuracy = train_and_validate(model, X_train, y_train, X_val, y_val)
    print("Validation Accuracy:", val_accuracy)

    # Save model
    save_model(model, model_dir)

    # Zip model to share it
    zip_directory(model_dir, MODEL_Dis_File)

    # Get Google Drive link for the saved model
    model_gdrive_link = get_gdrive_link(MODEL_Dis_File)
    get_shareable_link(model_gdrive_link)

    return model_gdrive_link

"""Now we test the model"""

import os
import zipfile
import pandas as pd
from sklearn.metrics import accuracy_score
import joblib

def download_zip_file_from_link(gdrive_link, destination_path):
    # Code to download the model file from the given link
    pass

def unzip_file(zip_file_path, destination_path):
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(destination_path)

def test_dis(test_file, MODEL_PATH, model_gdrive_link):
    """
    take test_file, model_file and output_dir as input.
    It loads model and tests on the examples in the test_file.
    It prints different evaluation metrics and saves the output in the output directory.

    Args:
        test_file: test file name
        model_gdrive_link: GDrive URL
        MODEL_PATH: Directory of Model
    """
    print('\nStart by downloading model')
    # Download and unzip the model file
    test_model_file = os.path.join(MODEL_PATH, 'test.zip')
    test_model_path = os.path.join(MODEL_PATH, 'test')
    download_zip_file_from_link(model_gdrive_link, test_model_file)
    print('Model downloaded to', test_model_file)
    unzip_file(test_model_file, test_model_path)
    print('Model is downloaded to', test_model_path)

    # Load the trained model
    model_path = os.path.join(test_model_path, 'svm_model.pkl')
    loaded_model = joblib.load(model_path)

    # Load test data
    test_df = pd.read_csv(test_file)

    # Tokenize and vectorize test data
    vectorizer = TfidfVectorizer()
    X_test = vectorizer.transform(test_df['comment'])

    # Perform inference
    y_pred = loaded_model.predict(X_test)

    # Save the model output in the test file
    test_df['out_label_model_Dis'] = y_pred
    test_df.to_csv(test_file, index=False)
    print('Output is saved in', test_file)

    return test_file